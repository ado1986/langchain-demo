{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf14978-538a-44f3-b555-09feda684623",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd83051-ff88-426d-b855-ed0d42d0daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5806e904-c630-443c-986f-6c622f960957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# è¯•è¯•qwen3-maxçš„ç»“æ„åŒ–è¾“å‡ºæ•ˆæœï¼Œæ¨¡å‹qwen-maxçš„æ•ˆæœä¸è¡Œ\n",
    "model = init_chat_model(\"openai:qwen3-max\",\n",
    "                        temperature=0.5,\n",
    "                        timeout=10,\n",
    "                        max_tokens=1000,\n",
    "                        base_url=os.getenv('BASE_URL'),\n",
    "                        api_key=os.getenv('API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfe0564-bd05-485e-81fa-d8427deba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# åœ¨ç»“æ„åŒ–è¾“å‡ºä¸Šï¼Œdeepseekçš„è¡¨ç°è¦æ¯”qwen-maxå¼ºã€‚ä½¿ç”¨qwen3-maxæ¨¡å‹çš„æ•ˆæœä¹Ÿokï¼Œæ”¯æŒç»“æ„åŒ–è¾“å‡ºã€‚\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e3c863-e685-407f-abc2-90e6e12c1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8caf7d40-dcef-4c90-aabf-6c44bfb87898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFormat(punny_response='Floridaâ€™s shining bright againâ€”looks like the sunâ€™s *on a roll*! Donâ€™t forget your sunscreen unless you want to be a crispy critter!', weather_conditions='sunny')\n",
      "ResponseFormat(punny_response=\"You're welcome! Keep riding those sunny vibesâ€”just donâ€™t *solar* your responsibilities! ğŸ˜\", weather_conditions='sunny')\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ToolStrategy(ResponseFormat),\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "# agent = create_agent(\n",
    "#     model = \"deepseek-chat\",\n",
    "#     tools=[get_user_location, get_weather_for_location],\n",
    "#     system_prompt=SYSTEM_PROMPT,\n",
    "#     context_schema=Context,\n",
    "#     response_format=ToolStrategy(ResponseFormat),\n",
    "#     checkpointer=checkpointer\n",
    "# )\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config = config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
